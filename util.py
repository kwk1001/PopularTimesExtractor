#!/usr/bin/env python

from datetime import datetime
from selenium import webdriver
from selenium.common.exceptions import *
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service as ChromeService
from webdriver_manager.chrome import ChromeDriverManager
from openlocationcode import openlocationcode as olc
from tqdm import tqdm
import json
import time
import re
import os

# gmaps starts their weeks on sunday
days = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']

def initialise_driver():
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--window-size=1920,1080")
    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=chrome_options)
    driver.implicitly_wait(5)
    return driver

def pprint_times(times):
    for i, day in enumerate(days):
        print(day, times[i])

def click(driver, elem):
    try:
        elem.click()
    except:
        driver.execute_script("arguments[0].click();", elem)

def extract_place(driver, features, name, link):
    try:
        approx_ll = re.search(f'(?P<lat>-?\d+\.\d+).+?(?P<lng>-?\d+\.\d+)', link).groupdict()
        lat = float(approx_ll["lat"])
        lng = float(approx_ll["lng"])
    except AttributeError:
        print(f"No approx latlong in URL {link} for {name}")
        return
    try:
        full_label = driver.find_element(By.CSS_SELECTOR, "button[aria-label^='Plus code:']").get_attribute("aria-label")
        code = full_label.split(":")[-1].strip() 
        print(f"Plus code: {code}")
        codeArea = olc.decode(olc.recoverNearest(code.split()[0], lat, lng))
    except NoSuchElementException:
        print("No plus code, latlong might be inaccurate")
        code = None
    except StaleElementReferenceException:
        # Try again
        print("Got a StaleElementReferenceException when trying to get the plus code, trying again")
        time.sleep(.1)
        return extract_place(driver, features, name, link)
    driver.implicitly_wait(.1)
    address = None
    try:
        address = driver.find_element(By.CSS_SELECTOR, "button[data-tooltip='Copy address']").get_attribute("aria-label").split(":")[-1].strip()
    except NoSuchElementException:
        pass
    category = None
    try:
        category = driver.find_element(By.CSS_SELECTOR, "button[jsaction='pane.rating.category']").text
    except NoSuchElementException:
        pass
    live_info = None
    try:
        popular = driver.find_element(By.CSS_SELECTOR, "div[aria-label^='Popular times']")
        print("âœ… Has popular times")
        times = [[0]*24 for _ in range(7)] 
        
        # === æ ¸å¿ƒä¿®å¤ï¼šæŒ‰å¤©æŸ¥æ‰¾ ===
        # 1. æ‰¾åˆ°æ‰€æœ‰ 7 ä¸ªä»£è¡¨â€œå¤©â€çš„å®¹å™¨
        daily_containers = popular.find_elements(By.CSS_SELECTOR, "div.g2BVhd")
        print(f"ğŸ•µï¸  [è°ƒè¯•] æ‰¾åˆ°äº† {len(daily_containers)} ä¸ª 'å¤©' çš„å®¹å™¨ã€‚")

        if len(daily_containers) != 7:
            print(f"âŒ [è°ƒè¯•] è­¦å‘Šï¼šæ²¡æœ‰æ‰¾åˆ° 7 ä¸ª 'å¤©' çš„å®¹å™¨ï¼ŒHTML ç»“æ„å¯èƒ½å·²æ”¹å˜ï¼")

        # 2. éå†æ¯ä¸€å¤©çš„å®¹å™¨
        for dow, day_container in enumerate(daily_containers):
            print(f"\n--- [è°ƒè¯•] æ­£åœ¨å¤„ç† Day {dow} ({days[dow]}) ---")
            hour_prev = -1 # åœ¨æ¯å¤©å¼€å§‹æ—¶é‡ç½® hour_prev
            
            # 3. åœ¨å½“å‰â€œå¤©â€çš„å®¹å™¨å†…æŸ¥æ‰¾æ‰€æœ‰å¸¦ aria-label çš„å…ƒç´ 
            elements_in_day = day_container.find_elements(By.CSS_SELECTOR, "div[aria-label]")
            print(f"  [è°ƒè¯•] åœ¨ Day {dow} ä¸­æ‰¾åˆ°äº† {len(elements_in_day)} ä¸ª aria-label å…ƒç´ ã€‚")

            # 4. éå†å½“å¤©å†…çš„å…ƒç´ å¹¶è§£æ
            for i, elem in enumerate(elements_in_day):
                current_label = elem.get_attribute("aria-label")
                # print(f"    [è°ƒè¯•] å¤„ç†å…ƒç´  {i}: {current_label}") # å¯ä»¥é™éŸ³

                try:
                    # --- ä½¿ç”¨æœ€ç»ˆç‰ˆ RegEx è§£æ ---
                    
                    # æ¨¡å¼1: "Currently..." (å®æ—¶æ–‡æœ¬ï¼Œç”¨äºå¡«è¡¥ç©ºç¼º)
                    current_pattern = re.search(
                        r"^Currently (?P<live_percent>\d+)% busy, usually (?P<percent>\d+)% busy\.",
                        current_label, re.IGNORECASE
                    )
                    
                    # æ¨¡å¼2: "5% busy at 4 AM." (æ™®é€šæŸ±)
                    bar_pattern = re.search(
                        r"^(?P<percent>\d+)% busy at (?P<hour>\d+)\s+(?P<am_pm>[ap]m?\.?)",
                        current_label, re.IGNORECASE
                    )
                    
                    # æ¨¡å¼3: "Not busy at 1 AM." (ä¸ç¹å¿™æŸ±)
                    not_busy_pattern = re.search(
                        r"^Not busy at (?P<hour>\d+)\s+(?P<am_pm>[ap]m?\.?)",
                        current_label, re.IGNORECASE
                    )

                    # æ¨¡å¼4: "Live: ... at 7 PM." (å¸¦å°æ—¶çš„å®æ—¶æŸ±ï¼Œå¤‡ç”¨)
                    live_bar_pattern = re.search(
                        r"Live: (?P<live_percent>\d+)% busy, usually (?P<percent>\d+)% busy at (?P<hour>\d+)\s+(?P<am_pm>[ap]m?\.?)",
                        current_label, re.IGNORECASE
                    )

                    percent_val = 0
                    hour_val = None
                    am_pm_val = None
                    is_live_text = False 

                    if current_pattern:
                        # print(f"    [è°ƒè¯•] âœ… åŒ¹é…åˆ° 'Currently' æ–‡æœ¬")
                        hour_val = hour_prev + 1 # æ¨æ–­å°æ—¶
                        percent_val = int(current_pattern.group("percent"))
                        is_live_text = True
                        
                        live_info = { # ä¿å­˜ Live Info
                            "live_frequency": int(current_pattern.group("live_percent")),
                            "usual_frequency": percent_val, "day": days[dow], "hour": hour_val
                        }

                    elif live_bar_pattern:
                         # print(f"    [è°ƒè¯•] âœ… åŒ¹é…åˆ° 'Live' æŸ±")
                         percent_val = int(live_bar_pattern.group("percent"))
                         hour_val = int(live_bar_pattern.group("hour"))
                         am_pm_val = live_bar_pattern.group("am_pm")
                         live_info = { # ä¿å­˜ Live Info
                            "live_frequency": int(live_bar_pattern.group("live_percent")),
                             "usual_frequency": percent_val, "day": days[dow]
                         }

                    elif bar_pattern:
                        # print(f"    [è°ƒè¯•] âœ… åŒ¹é…åˆ° 'æ™®é€š' æŸ±")
                        percent_val = int(bar_pattern.group("percent"))
                        hour_val = int(bar_pattern.group("hour"))
                        am_pm_val = bar_pattern.group("am_pm")

                    elif not_busy_pattern:
                        # print(f"    [è°ƒè¯•] âœ… åŒ¹é…åˆ° 'Not Busy' æŸ±")
                        percent_val = 0
                        hour_val = int(not_busy_pattern.group("hour"))
                        am_pm_val = not_busy_pattern.group("am_pm")

                    else:
                        # print(f"    [è°ƒè¯•] âš ï¸ å¿½ç•¥éæ•°æ®æ ‡ç­¾: {current_label}")
                        continue # è·³åˆ°ä¸‹ä¸€ä¸ª 'for' å¾ªç¯

                    # --- è§£æå°æ—¶ ---
                    if not is_live_text: # "Currently" æ–‡æœ¬çš„å°æ—¶å·²æ˜¯ 24h åˆ¶
                        if hour_val == 12: hour_val = 0
                        if am_pm_val.lower().startswith("p"): hour_val += 12
                    
                    # --- æ›´æ–° hour_prev (åªåœ¨è§£ææˆåŠŸå) ---
                    hour_prev = hour_val 

                    # --- å­˜å…¥æ•°æ® ---
                    if dow < 7: # ä½¿ç”¨å¤–å±‚å¾ªç¯çš„ dow
                        # print(f"      [è°ƒè¯•] æˆåŠŸ! å­˜å…¥: Day {dow}, Hour {hour_val}, Value {percent_val}")
                        times[dow][hour_val] = percent_val # ç›´æ¥ä½¿ç”¨ dow
                
                except Exception as e:
                    print(f"  âŒ [è°ƒè¯•] å†…å¾ªç¯å‡ºé”™: {e}")
                    print(f"     -> æ— æ³•è§£ææ ‡ç­¾: {current_label}")

    except NoSuchElementException:
        print("No popular times available")
        times = None
    except StaleElementReferenceException:
        print("Got a StaleElementReferenceException when trying to get the popular times, trying again")
        time.sleep(.1)
        return extract_place(driver, features, name, link)
    except Exception as e: # æ•è·å…¶ä»–æœªçŸ¥é”™è¯¯
        print(f"âŒ [è°ƒè¯•] å¤–å¾ªç¯æˆ–æŸ¥æ‰¾ 'popular' æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        times = None
    feature = {
        "type": "Feature",
        "geometry": {
            "type": "Point",
            "coordinates": [lng, lat]
        },
        "properties": {
            "name": name,
            "address": address,
            "category": category,
            "link": link,
            "code": code,
            "live_info": live_info,
            "populartimes": times,
            "scraped_at": datetime.now().isoformat(sep=" ", timespec="seconds")
        }
    }
    #print(feature)
    features[link] = feature
    driver.implicitly_wait(5)

def refreshPlaces(driver):
    places = []
    scrollCount = 0
    while len(places) < 120 and scrollCount < 10:
        scrollCount += 1
        print("scrolling")
        driver.execute_script("arguments[0].scrollTo(0, arguments[0].scrollHeight)", driver.find_element(By.CSS_SELECTOR, "div[role='feed']"))
        time.sleep(1)
        places = driver.find_elements(By.CSS_SELECTOR, "div[role='feed'] a[aria-label]")
    if not places:
        print("No places")
        raise IndexError
    return places

def extract_page(driver, features):
    try:
        places = refreshPlaces(driver)
    except NoSuchElementException:
        # Single result
        name = driver.find_element(By.CSS_SELECTOR, "h1").text
        print(f"Found {name}")
        link = driver.current_url
        if link in features:
            print(f"Skipping {name}")
        else:
            extract_place(driver, features, name, link)
        return 1

    for place in tqdm(places):
        name = place.get_attribute('aria-label')
        link = place.get_attribute("href")
        if name.startswith("Ad Â·"):
            # Don't click on Ads
            continue
        if link in features:
            print(f"Skipping {name}")
            continue
        print(f"Clicking on {name}")
        click(driver, place)
        extract_place(driver, features, name, link)
    return len(places)

def load(features, OUTFILE):
    if os.path.isfile(OUTFILE):
        # Load existing data
        with open(OUTFILE) as f:
            data = json.load(f)
            for feature in data["features"]:
                features[feature["properties"]["link"]] = feature
            print(f"Loaded {len(features)} features")

def save(features, OUTFILE):
    if features:
        geojson = {
            "type": "FeatureCollection",
            "features": list(features.values())
        }

        with open(OUTFILE, "w") as f:
            json.dump(geojson, f)
        print(f"Wrote {len(features)} places")